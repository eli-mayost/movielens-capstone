---
title: "Capstone project - MovieLens"
author: "Eli F Mayost"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    theme: yeti
  pdf_document: default
always_allow_html: yes
---

```{r setup, include=FALSE}
if(!require("knitr")) install.packages("knitr")
if(!require("lubridate")) install.packages("lubridate")
if(!require("ggplot2")) install.packages("ggplot2")
if(!require("vroom")) install.packages("vroom")
if(!require("stringr")) install.packages("stringr")
```

<!-- Style for various components -->
<style type="text/css">
.title{
  font-weight: bold;
}
.author {
  margin-top: 50px;
  font-size: 1.1em;
  font-weight: bold;
}
.date {
  font-size: 1.1em;
}
body {
  font-size: 14px;
}
h2 {
  font-size: 1.5em;
  font-weight: bold;
  margin-top: 100px;
  margin-bottom: 50px;
}
h3 {
  font-size: 0.9em;
  font-weight: bold;
  margin-top: 100px;
  margin-bottom: 50px;
}
pre {
  font-size: 0.9em;
  margin-top: 20px;
  margin-bottom: 40px;
  padding: 20px;
}
.table {
  font-size: 0.9em;
  margin-top: 50px;
  margin-bottom: 50px;
}
.plotly {
  margin-top: 50px;
  margin-bottom: 50px;
}
img {
  margin-bottom: 50px;
}
.footnotes {
  font-size: 0.9em;
  margin-top: 20px;
  margin-bottom: 20px;
}
</style>

\newpage
## Introduction
In this project, we will develop a recommendation model for movies.

Recommendation models are useful, to companies that sell products and wish to maximize their benefits by recommending similar items based on other user's buys.

It is also useful to their customers to be able to discover what like minded people consume.

A concrete example, is the recommendations a user get for books on Amazon, or movies on Netflix.

\newpage
## Aim
The target of this project is to use regression to minimize RMSE. Both training (edx here) and test (validation) sets are provided.

We will rate movies from 0.5 to 5, with steps of 0.5, keeping in mind that we are asked to minimize the RMSE [^1], between the predicted value and the observed value, to be lower than, or equal to, 0.8775 (the lower the better).

[^1]: RMSE. Root Mean Square Error. Can simply be written as sqrt((prediction - observation)^2). We will apply it to vectors using the RMSE function in the caret package.    

\newpage
## The data
The MovieLens 10 million dataset is automatically downloaded and split into a training subset, called edx, and a validation subset called validation in a variables carying these names.

The following code to achieve these partitions is given by the course's staff:

```{r message = FALSE}
################################
# Create edx set, validation set
################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data

set.seed(1)

test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

\newpage
## Data wrangling

Checking that we do not have missing values in both sets: \newline

```{r}
edx %>% is.na() %>% any()
validation %>% is.na() %>% any()
```

Verifying that the ratings are indeed between 0.5 and 5 with 0.5 steps:

```{r}
edx %>% count(rating, name = "total")
validation %>% count(rating, name = "total")
```

Both datasets have 6 features [^2] with the following number of observations:

```{r}
edx %>% dim()
validation %>% dim()
```

[^2]: A unique user ID (userId).\
A unique movie ID (movieId).\
A rating (0.5 to 5 with 0.5 step).\
A timestamp (Unix epoch).\
A title (includes film's year).\
A list of genres (separated by a pipe).

\newpage
## Data analysis (edx set) 

The first 10 observations:

```{r}
edx %>% head(10) %>% kable()
```

The number of unique users, and unique movies:

```{r}
tibble(users = length(unique(edx$userId)), movies = length(unique(edx$movieId))) %>%
    kable()
```

### Rating distribution: 

```{r}
edx %>%
    mutate(rating = as.factor(rating)) %>% 
    ggplot(aes(x = rating, color = rating, fill = rating)) + 
        geom_bar() + 
        scale_y_discrete(name = "total", labels = scales::comma, limits = seq(0, 3000000, 500000)) + 
        theme_minimal()
```

The rating distributon is right skewed with ratings 3, 4 and 5 used more often.

### Genres distribution: 

Tydyverse, separate_rows was very slow, therefore, I have used linux sed, awk and tr to split the genres (about 20 seconds).

If we were to do it with tidyverse:

<pre>
read_table(edx$genres, col_names=c("types")) %>%
    separate_rows(types, sep = "\\|") %>% 
    count(types, name = "total", sort = T)
</pre>


```{r}
edx %>% select(genres) %>% vroom_write(path = "genres.csv", delim = ",")

genres_df <- read.table(pipe("sed '1d' genres.csv | tr '|' '\n' | sort | uniq -c | awk '{OFS=\",\"; print $2, $1}'"),
                        sep = ",", 
                        col.names = c("genre", "total"))

genres_df %>% kable()
```

We remove the rows without genre, and graph the genres distribution:

```{r}
genres_df %>% 
  filter(! str_detect(genre, "\\(no")) %>% 
  ggplot(aes(x = genre, fill = genre)) + 
    geom_bar(aes(y = total), stat = "identity") + 
    scale_x_discrete(name = "genre") +
    scale_y_continuous(name = "total", labels = scales::comma) + 
    coord_flip() +
    theme_minimal() +
    theme(legend.position = "none", axis.text.x = element_text(angle = -90))
```

Number of ratings per year, adding a column with the year the movie was rated:

```{r}
edx <- edx %>%
    mutate(year_rated = lubridate::year(as.Date(as.POSIXct(timestamp, origin="1970-01-01"))))

edx %>% count(year_rated, name = "total")
```

As 1995 has only two ratings, we can remove 1995:

```{r}
edx <- edx %>% filter(! year_rated == 1995)

edx %>% count(year_rated, name = "total")
```

Let's see the distribution of total of rating per user:

```{r}
edx %>% count(userId) %>% 
  ggplot(aes(n)) + 
    geom_histogram(bins = 30, color = "white", fill = "steelblue") + 
    scale_x_log10() +
    theme_minimal() +
    theme(legend.position = "none", axis.text.x = element_text(angle = -90))
```

We can see that some users are very prolific and have rated many movies, and that some users have rated very few movies.

We will take this bias into account when modelling and call it the "user activity bias".

Let's do the same with the movie IDs:

```{r}
edx %>% count(movieId) %>% 
  ggplot(aes(n)) + 
    geom_histogram(bins = 30, color = "white", fill = "steelblue") + 
    scale_x_log10() +
    theme_minimal() +
    theme(legend.position = "none", axis.text.x = element_text(angle = -90))
```

Same here, some movies are more rated than others, and that will be taken into account during modelling under the name "movie bias".

\newpage
## Modelling

### Simplest possible model

The simplest model is to predict the average rating for every movie:

```{r}
avg <- mean(edx$rating)
rmse_simple <- caret::RMSE(pred = avg, obs =validation$rating)
rmse_simple
```

The RMSE of this method is `r rmse_simple`.

### Introducing user activity bias

The user activity bias is calculated as the average rating by user minus the average rating of the whole edx set:

```{r}
user_bias <- edx %>% 
  group_by(userId) %>% 
  summarise(ub = mean(rating - avg))

data <- validation %>% select(userId, rating) %>% left_join(user_bias, by = 'userId') %>% mutate(pred= avg + ub) 

user_bias_rmse <- RMSE(pred = data$pred, obs = data$rating)

user_bias_rmse
```

As there are users who have a very low number of ratings, let's try and get a number of ratings under which we will exclude these users:

```{r}
limits <- seq(50, 200, 50)

rmses <- sapply(limits, function(limit){
  users <-
    edx %>% count(userId, name = "total") %>%
    filter(total >= limit)
  
  user_bias <- edx %>%
    filter(userId %in% users$userId) %>%
    group_by(userId) %>% 
    summarise(ub = mean(rating - avg))
  
  data <- validation %>%
    filter(userId %in% users$userId) %>%
    select(userId, rating) %>% 
    left_join(user_bias, by = 'userId') %>% 
    mutate(pred= avg + ub) 
  
  RMSE(pred = data$pred, obs = data$rating)
})

tibble(limit = limits, rmse = rmses) %>%
  ggplot(aes(x = limit)) + 
  geom_point(aes(y = rmse), colour = "steelblue") +
  theme_minimal()
```

The RMSE is lowest when we filter away the users with less that 150 ratings, so we apply this and can see a slight improvement in our RMSE:

```{r}
users <-
  edx %>% count(userId, name = "total") %>%
  filter(total >= 150)

user_bias <- edx %>%
  filter(userId %in% users$userId) %>%
  group_by(userId) %>% 
  summarise(ub = mean(rating - avg))

data <- validation %>%
  filter(userId %in% users$userId) %>%
  left_join(user_bias, by = 'userId') %>% 
  mutate(pred= avg + ub) 

user_bias_rmse <- RMSE(pred = data$pred, obs = data$rating)

user_bias_rmse
```

Let's calculate a movie bias:

```{r}
movie_bias <- edx %>% 
  group_by(movieId) %>% 
  summarise(mb = mean(rating - avg))

data <- validation %>%
  left_join(movie_bias, by = 'movieId') %>%
  mutate(pred = avg + mb) 

movie_bias_rmse <- RMSE(pred = data$pred, obs = data$rating)

movie_bias_rmse
```

We will combine both biases:

```{r}
data <- validation %>% 
  select(userId, movieId, rating) %>% 
  left_join(movie_bias, by = 'movieId') %>% 
  left_join(user_bias, by = 'userId') %>% 
  mutate(pred = avg + ub + mb) 

user_movie_bias_rmse <- RMSE(pred = data$pred, obs = data$rating, na.rm = T)
user_movie_bias_rmse
```

The RMSE (`r user_movie_bias_rmse`) is already under the target RMSE so no further improvement is needed.

\newpage
## Conclusion:

I have achieved the goal of the project (RMSE < 0.8649 for full points), and therefore will not continue lowering the RMSE, but further improvements can be gained by defining a coeficient for each user so we have a weighted average based on his ratings' total compared to the total of ratings.

All of that will be achieved depending on CPU and RAM limits, and as with every improvement in accuracy, we need to wwight the spend on hardware against the difference in accuracy gained.



